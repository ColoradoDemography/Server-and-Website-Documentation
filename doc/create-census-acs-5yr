Create database in postgres (ie acs0711)
Add postgis extension to database
Create carto, data, search and tiger schemas within database
Download and load geographies 
*TIGER geographies can be downloaded here: https://www2.census.gov/geo/tiger/TIGER<year> (don't load tiger unless necessary)
*CARTO geographies can be downloaded here: https://www2.census.gov/geo/tiger/GENZ<year>
State and county boundaries can be reused barring change

Pull data using the ACS_Download R Project. Run Get_ACS_Tables.R in pieces.
Use find and replace to change all variables with a year to the year desired.

Run the vars variables, removing columns that are not in a B#### or C#### format (BLKGRP for example) (all columns seem to be valid in 2020)
Run acs500 function to store it.
Run acs500("us") to get the variables for data for the entire United States
Run acs500("state") to get variables for each state
Run acs500("county") to get the variables for each county 
Rbind the 3 tables together
  
Use the table manipulation functions to prepare the data, then write to postgres
Backup the data schema.

Run acs500("place") overnight and save it to a csv. Use csv splitter to split it into ~10 files of 3000 lines each.

Load each csv and run the loop to append these files to the tables.
 
*hopefully delete what is below
Run acs500("place") to get the data for those geographies and run the table manipulation loop (getdata)
to append those geographies to the data in postgres.
Save the data schema again with a new name.

For tracts, run the acs500tract function.  Then run the following loop to start writing tract csvs. This will have to be restarted a few
times with the state_codes variable adjusted according to the states remaining.

Once the csvs are written, those that are over 500 mb will need to be split with the csv splitter.

Now make sure the only csvs in the working directory are the tracts and split tracts (make sure the full csvs for tracts that had to be split
are removed. Now run the final large loop at the end to append the csv data into the database. This will take all day but should run without
crashing. Save the database again.

For block groups, we only do Colorado. Run the acs500bg function, then run the loop to get all the block group data write it to csvs (will take a day). 
Then run the csv loop to read all of them and load the data. Backup the database, hopefully for the last time.

The CheckData script then needs to be run to find errors in the tables. There will be some mistakes given the way the data has to be split up when loaded.
SQL scripts have to be run to remove the bad data and then individual tables loaded up with the correct data (append script around line 223)

Other tables to update are data.census_table_metadata, data.census_column_metadata, data.geo, search.data, search.data_exp and search.key. The first two will have
slight changes as the Census Bureau adds and removes tables and columns. This data can be found in the ACS Table Shells. The other tables can be copied from the 
previous year except after the decennial.

---Updating the API
Several files in github need to be updated, usually just adding new database info
MS_CensusMap - demogpost.js
MS_CensusAPI - demog.js, geojson.js, meta.js
CensusAPI - index.html

--New Census Map
Clone the previous year's map. The following files will need to be updated:
charttree.js
colortree.js
initialize_cMap.js
tableflavor.js

In 2023, try using the AddTable script to add the tables one at a time without the splits. Loop through the table list (census_table_metadata)

